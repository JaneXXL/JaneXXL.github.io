<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Paper-reading</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-04-23T12:40:38.131Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>xiaoJane</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>文本分类</title>
    <link href="http://yoursite.com/2018/04/27/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/04/27/文本分类/</id>
    <published>2018-04-27T01:34:18.403Z</published>
    <updated>2018-04-23T12:40:38.131Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h2><p>文本分类特性：<code>高维性</code>和<code>向量疏稀性</code></p><p>特征提取：<code>特征选择</code>和<code>特征抽取</code></p><p>对同一个分类问题，用同样的文档模型，但因为关注数据不同方面的特性而可能得到不同的结论。</p><p>分类算法：决策树，Rocchio，朴素贝叶斯，神经网络，支持向量机，线性最小平方拟合，kNN，遗传算法，最大熵，Generalized Instance Set</p><hr><h4 id="Rocchio算法"><a href="#Rocchio算法" class="headerlink" title="Rocchio算法"></a>Rocchio算法</h4><p>基本的思路是把一个类别里的样本文档各项取个平均值（例如把所有“体育”类文档中词汇“篮球”出现的次数取个平均值，再把“裁判”取个平均值，依次做下去），可以得到一个新的向量，形象的称之为“质心”，有新文档需要判断的时候，比较新文档和质心间的距离，就可以确定新文档属不属于这个类。</p><p>Rocchio算法做了两个很致命的假设，使得它的性能出奇的差。</p><ul><li>它认为一个类别的文档仅仅聚集在一个质心的周围，实际情况往往不是如此（这样的数据称为线性不可分的）</li><li>它假设训练数据是绝对正确的，因为它没有任何定量衡量样本是否含有噪声的机制，因而也就对错误数据毫无抵抗力。</li></ul><p>常常被用来做科研中比较不同算法优劣的基线系统（Base Line）。</p><hr><h4 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h4><p>贝叶斯算法关注的是文档属于某类别概率。朴素贝叶斯算法的公式：</p><p>$P(C_i|d)=\frac{P(d|C_i)P(C_i)}{P(d)}$    $其中,P(d|C_i)=P(w_1|C_i)P(w_2|C_i)…P(w_i|C_i)P(w_{i+1}|C_i)…P(w_m|C_i)$</p><p>$P(w_i|C_i)$就代表词汇wi属于类别Ci的概率</p><p>朴素贝叶斯算法两大缺陷：</p><ul><li>P(d| Ci)之所以能展开成（式1）的连乘积形式，就是假设一篇文章中的各个词之间是彼此独立的，其中一个词的出现丝毫不受另一个词的影响，但这显然不对，即使不是语言学专家的我们也知道，词语之间有明显的所谓“共现”关系，在不同主题的文章中，可能共现的次数或频率有变化，但彼此间绝对不是独立。</li><li>使用某个词在某个类别训练文档中出现的次数来估计P(wi|Ci)时，只在训练样本数量非常多的情况下才比较准确。这需要大量样本的要求不仅给前期人工分类的工作带来更高要求，在后期由计算机处理的时候也对存储和计算资源提出了更高的要求。</li></ul><hr><h4 id="KNN算法"><a href="#KNN算法" class="headerlink" title="KNN算法"></a>KNN算法</h4><p>基本思想是在给定新文档后，计算新文档特征向量和训练文档集中各个文档的向量的相似度，得到K篇与该新文档距离最近最相似的文档，根据这K篇文档所属的类别判定新文档所属的类别。</p><p>这种判断方法很好的克服了Rocchio算法中无法处理线性不可分问题的缺陷，也很适用于分类标准随时会产生变化的需求。</p><p>kNN唯一的也可以说最致命的缺点就是判断一篇新文档的类别时，需要把它与现存的所有训练文档全都比较一遍，这个计算代价并不是每个系统都能够承受的</p><hr><h4 id="SVM算法"><a href="#SVM算法" class="headerlink" title="SVM算法"></a>SVM算法</h4><p>它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。SVM 训练的本质是解决一个二次规划问题（Quadruple Programming，指目标函数为二次函数，约束条件为线性约束的最优化问题），得到的是全局最优解。</p><p>SVM 分类器的文本分类效果很好，是最好的分类器之一。</p><p>同时使用核函数将原始的样本空间向高维空间进行变换，能够解决原始样本线性不可分的问题。</p><p>其<code>缺点</code>是核函数的选择缺乏指导，难以针对具体问题选择最佳的核函数；另外SVM 训练速度极大地受到训练集规模的影响，计算开销比较大，针对SVM 的训练速度问题，研究者提出了很多改进方法，包括Chunking 方法、Osuna 算法、SMO 算法和交互SVM 等等</p><hr><h3 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h3><p>给定一个带约束的优化问题</p><p>​        目标函数：min f(x)</p><p>​        约束条件：C(x) ≥0</p><p>可以通过拉格朗日乘子构造拉格朗日函数</p><p>​        L(x,λ)=f(x)- λTC(x)</p><p>令g(λ)= f(x)- λTC(x)</p><p>则原问题可以转化为</p><p>​        目标函数：max g(λ)</p><p>​        约束条件：λ≥0</p><p>这个新的优化问题就称为原问题的对偶问题（两个问题在取得最优解时达到的条件相同）。</p><hr><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>特征选择算法有互信息，文档频率，信息增益，开方检验等等十数种</p><h4 id="开方检验"><a href="#开方检验" class="headerlink" title="开方检验"></a>开方检验</h4><p>开方检验其实是数理统计中一种常用的检验两个变量独立性的方法，通过观察实际值与理论值的偏差来确定理论的正确与否。</p><p>$\sum_{i=1}^{n}\frac{(x_i-E)^2}{E}$</p><p>先假设两个变量确实是独立的，然后观察实际值与理论值的偏差程度。如果偏差足够小，我们就认为误差是很自然的样本误差，是测量手段不够精确导致或者偶然发生的，两者确确实实是独立的，此时就接受原假设；如果偏差大到一定程度，使得这样的误差不太可能是偶然产生或者测量不精确所致，我们就认为两者实际上是相关的，即否定原假设，而接受备择假设。</p><p><code>举例：考察一个词“篮球”与类别“体育”之间的相关性</code></p><ol><li>包含“篮球”且属于“体育”类别的文档数，命名为A</li></ol><ol start="2"><li>包含“篮球”但不属于“体育”类别的文档数，命名为B</li><li>不包含“篮球”但却属于“体育”类别的文档数，命名为C</li><li>既不包含“篮球”也不属于“体育”类别的文档数，命名为D</li></ol><table><thead><tr><th>特征选择</th><th style="text-align:left">属于体育</th><th style="text-align:left">不属于体育</th><th style="text-align:left">总计</th></tr></thead><tbody><tr><td>包含“篮球”</td><td style="text-align:left">A</td><td style="text-align:left">B</td><td style="text-align:left">A+B</td></tr><tr><td>不包含篮球</td><td style="text-align:left">C</td><td style="text-align:left">D</td><td style="text-align:left">C+D</td></tr><tr><td>总数</td><td style="text-align:left">A+C</td><td style="text-align:left">B+D</td><td style="text-align:left">N</td></tr></tbody></table><p>A+B+C+D=N</p><p>如果原假设是成立的，即“篮球”和体育类文章没什么关联性，那么在所有的文章中，“篮球”这个词都应该是等概率出现，而不管文章是不是体育类的。概率接近：$\frac{A+B}{N}$</p><p>应该有$E_n=(A+C)\frac{A+B}{N}$篇包含”篮球”这个词的体育文</p><p>差值：$D_n=\frac{(A-E_n)^2}{E_n}$</p><p>计算“篮球”与“体育”类文章的开方值：$\chi^2(篮球，体育)=D_{11}+D_{12}+D_{21}+D_{22}$</p><p>化简：$\chi^2(篮球，体育)=\frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}$</p><h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>在信息增益中，重要性的衡量标准就是看特征能够为分类系统带来多少信息，带来的信息越多，该特征越重要。</p><p>说有这么一个变量X，它可能的取值有n多种，分别是x1，x2，……，xn，每一种取到的概率分别是P1，P2，……，Pn，那么X的熵就定义为：</p><p>$H(X)=-\sum_{i=1}^{n}P_ilog_2P_i$</p><p>一个变量可能的变化越多（反而跟变量具体的取值没有任何关系，只和值的种类多少以及发生概率有关），它携带的信息量就越大</p><p>对分类系统来说，类别C是变量，它可能的取值是C1，C2，……，Cn，而每一个类别出现的概率是P(C1)，P(C2)，……，P(Cn)，因此n就是类别的总数。此时分类系统的熵就可以表示为：</p><p>$H(C)=-\sum_{i=1}^{n}P(C_i)log_2P(C_i)$</p><p><code>信息增益</code>是针对一个的特征而言的，就是看一个特征t，系统有它和没它的时候信息量各是多少，两者的差值就是这个特征给系统带来的信息量，即增益。</p><p>系统含有特征t的时候信息量很好计算，就是刚才的式子，它表示的是包含所有特征时系统的信息量。当系统不包含t时，信息量如何计算？</p><p><em>把系统要做的事情想象成这样：说教室里有很多座位，学生们每次上课进来的时候可以随便坐，因而变化是很大的（无数种可能的座次情况）；但是现在有一个座位，看黑板很清楚，听老师讲也很清楚，于是校长的小舅子的姐姐的女儿托关系（真辗转啊），把这个座位定下来了，每次只能给她坐，别人不行，此时情况怎样？对于座次的可能情况来说，我们很容易看出以下两种情况是等价的：（1）教室里没有这个座位；（2）教室里虽然有这个座位，但其他人不能坐（因为反正它也不能参与到变化中来，它是不变的）。</em> </p><p>下面的等价：（1）系统不包含特征t；（2）系统虽然包含特征t，但是t已经固定了，不能变化。</p><p>我们计算分类系统不包含特征t的时候，就使用情况（2）来代替。这个信息量其实也有专门的名称，就叫做“条件熵”，条件嘛，自然就是指“t已经固定“这个条件。</p><p>当计算条件熵而需要把它固定的时候，要把它固定在哪一个值上呢？答案是每一种可能都要固定一下，计算n个值，然后取均值才是条件熵。</p><p><code>特征X被固定为值xi时的条件熵</code></p><p>$H(C|X=x_i)$</p><p><code>特征X被固定时的条件熵</code></p><p>$H(C|X)$</p><p>两式子的关系：</p><p>$H(C|X)=P_1H(C|X=x_1)+P_2H(C|X=x_2)+…+P_nH(C|X=x_n)=\sum_{i=1}^{n}P_iH(C|X=x_i)$</p><p>我们用T代表特征，而用t代表T出现，那么<code>固定t时系统的条件熵</code>：</p><p>$H(C|T)=P(t)H(C|t)+P(\overline{t})H(C|\overline{t})$</p><p>其中：</p><p>$H(C|t)=-\sum_{i=1}^{n}P(C_i|t)log_2P(C_i|t)$    </p><p>$H(C|\overline{t})=-\sum_{i=1}^{n}P(C_i|\overline{t})log_2P(C_i|\overline{t})$</p><p><code>因此特征T给系统带来的信息增益就可以写成系统原本的熵与固定特征T后的条件熵之差：</code></p><p>$IG(T)=H(C)-H(C|T)=-\sum_{i=1}^{n}P(C_i)log_2P(C_i)+\sum_{i=1}^{n}P(C_i|t)log_2P(C_i|t)+\sum_{i=1}^{n}P(C_i|\overline{t})log_2P(C_i|\overline{t})$</p><p>信息增益最大的问题还在于它只能考察特征对整个系统的贡献，而不能具体到某个类别上，这就使得它只适合用来做所谓“全局”的特征选择（指所有的类都使用相同的特征集合），而无法做“本地”的特征选择（每个类别有自己的特征集合，因为有的词，对这个类别很有区分度，对另一个类别则无足轻重）。</p><h4 id="特征权重"><a href="#特征权重" class="headerlink" title="特征权重"></a>特征权重</h4><p>特征，从人类能够理解的形式转换为计算机能够理解的形式时，实际上经过了两步骤的量化——特征选择阶段的重要程度量化和将具体文本转化为向量时的特征权重量化。</p><p>对卡方检验和信息增益这类方法来说，量化以后的得分越大的特征就越重要</p><p>在文本分类中，最常用的特征权重量化方法是TFIDF，最好把“权重“这个字眼忘掉，我们就把它说成计算得分好了，或者就仅仅说成是量化。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;文本分类&quot;&gt;&lt;a href=&quot;#文本分类&quot; class=&quot;headerlink&quot; title=&quot;文本分类&quot;&gt;&lt;/a&gt;文本分类&lt;/h2&gt;&lt;p&gt;文本分类特性：&lt;code&gt;高维性&lt;/code&gt;和&lt;code&gt;向量疏稀性&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;特征提取：&lt;code&gt;特
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>优化方法</title>
    <link href="http://yoursite.com/2018/04/27/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2018/04/27/优化方法/</id>
    <published>2018-04-27T01:34:18.403Z</published>
    <updated>2018-04-24T13:54:32.035Z</updated>
    
    <content type="html"><![CDATA[<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>最速下降法越接近目标值，步长越小，前进越慢。</p><p><img src="E:\Blog\photo\梯度下降法.png" alt="梯度下降法"></p><p>牛顿法的缺点：</p><p>　　（1）靠近极小值时收敛速度减慢，如下图所示；</p><p>　　（2）直线搜索时可能会产生一些问题；</p><p>　　（3）可能会“之字形”地下降。</p><p><img src="E:\Blog\photo\之.gif" alt="![img](file:///E:/Blog/photo/%E4%B9%8B.gif?lastModify=1524572482)之"></p><p>在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为<code>随机梯度下降法</code>和<code>批量梯度下降法</code>。</p><p>对一个线性回归（Linear Logistics）模型。其中m是训练集的样本个数，n是特征的个数。h(x)是要拟合的函数，J(θ)为损失函数，θ是参数。</p><p>$h(\theta)=\sum_{j=0}^{n}\theta_jx_j$</p><p>$J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(y^i-h_\theta(x^i))^2$</p><ul><li><p>批量梯度下降法（Batch Gradient Descent，BGD）</p><p>​    (1) $J(\theta)=\frac{1}{m}\sum_{i=1}^{m}\frac12(y^i-h_\theta(x^i))^2=\frac1m\sum_{i=1}^{m}\cos t(\theta,(x^i,y^i))$</p><p>​    $\frac {\partial J(\theta)} {\partial \theta_j}=-\frac{1}{m}\sum_{i=1}^{m}(y^i-h_\theta(x^i)){x_j}^i$</p><p>​     $cos t(\theta,(x^i,y^i))=\frac12(y^i-h_\theta(x^i))^2$</p><p>​    (2)${\theta_j}^{\prime}=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x_0^{(i)},x_1^{(i)},…,x_n^{(i)})-y_i){x_j}^{(i)}$</p><p>对于批量梯度下降法，样本个数m，x为n维向量，一次迭代需要把m个样本全部带入计算，迭代一次计算量为$m*n^2$。</p></li><li><p>小批量梯度下降法（Mini-batch Gradient Descent）</p><p>​    ${\theta_j}^{\prime}=\theta_j-\alpha\sum_{i=t}^{t+x-1}(h_\theta(x_0^{(i)},x_1^{(i)},…,x_n^{(i)})-y_i){x_j}^{(i)}$</p></li><li><p>随机梯度下降（Stochastic Gradient Descent，SGD）</p><p>​    ${\theta_j}^{\prime}=\theta_j-\alpha(h_\theta(x_0^{(i)},x_1^{(i)},…,x_n^{(i)})-y_i){x_j}^{(i)}$</p><p><code>批量梯度下降</code>—最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。</p><p><code>随机梯度下降</code>—最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。</p></li></ul><h2 id="牛顿法和拟牛顿法"><a href="#牛顿法和拟牛顿法" class="headerlink" title="牛顿法和拟牛顿法"></a>牛顿法和拟牛顿法</h2><p><code>牛顿法</code>是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。</p><p><code>步骤</code></p><p>$x_{(n+1)}\cdot f’(x_n)+f(x_n)-x_n\cdot f’(x_n)=0$</p><p>$x_{(n+1)}=x_n-\frac{f(x_n)}{f’(x_n)}$</p><p><img src="E:\Blog\photo\牛顿.gif" alt="牛顿"></p><p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。</p><p>牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面</p><p><img src="E:\Blog\photo\路径.png" alt="路径"></p><p>​                注：红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。</p><p>牛顿法的优缺点总结：</p><p>　　优点：二阶收敛，收敛速度快；</p><p>　　缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。</p><p><code>拟牛顿法</code>是求解非线性优化问题最有效的方法之一。本质是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。</p><p><code>步骤</code></p><p>首先构造目标函数在当前迭代$x_k$的二次模型：</p><p>$m_k(p)=f(x_k)+\bigtriangledown f(x_k)^Tp+\frac{p^TB_kp}{2}$</p><p>$p_k=-{B_k}^{-1}\bigtriangledown f(x_k)$</p><p>这里$B_k$是一个对称正定矩阵，于是我们取这个二次模型的最优解作为搜索方向，并且得到新的迭代点：</p><p>$x_{k+1}=x_k+a_kp_k$</p><p>现在假设得到一个新的迭代$x_{k+1}$，并得到一个新的二次模型：</p><p>$m_{k+1}(p)=f(x_{k+1})+\bigtriangledown f(x_{k+1})^Tp+\frac{p^TB_{k+1}p}{2}$</p><h2 id="共轭梯度法"><a href="#共轭梯度法" class="headerlink" title="共轭梯度法"></a>共轭梯度法</h2><h2 id="启发式优化方法"><a href="#启发式优化方法" class="headerlink" title="启发式优化方法"></a>启发式优化方法</h2><h2 id="解决约束优化问题-拉格朗日乘数法"><a href="#解决约束优化问题-拉格朗日乘数法" class="headerlink" title="解决约束优化问题-拉格朗日乘数法"></a>解决约束优化问题-拉格朗日乘数法</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;梯度下降法&quot;&gt;&lt;a href=&quot;#梯度下降法&quot; class=&quot;headerlink&quot; title=&quot;梯度下降法&quot;&gt;&lt;/a&gt;梯度下降法&lt;/h2&gt;&lt;p&gt;最速下降法越接近目标值，步长越小，前进越慢。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;E:\Blog\photo\梯度下降
      
    
    </summary>
    
      <category term="博客阅读" scheme="http://yoursite.com/categories/%E5%8D%9A%E5%AE%A2%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习列表</title>
    <link href="http://yoursite.com/2018/04/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%97%E8%A1%A8/"/>
    <id>http://yoursite.com/2018/04/27/机器学习列表/</id>
    <published>2018-04-27T01:34:03.846Z</published>
    <updated>2018-04-26T03:48:52.598Z</updated>
    
    <content type="html"><![CDATA[<p>#机器学习列表</p><p>参考链接<a href="https://www.cnblogs.com/mcafee/p/6bbddd792d25d1e0e2a8859729b028e9.html" target="_blank" rel="noopener">机器学习中的python</a></p><p>机器学习可以分为四组：分类，聚类，回归和降维。</p><p><img src="E:\Blog\photo\机器学习.png" alt="机器学习"></p><p>####1.机器学习的重要模块</p><p>机器学习最重要的模块是：<a href="http://www.numpy.org/" target="_blank" rel="noopener">NumPy</a>, <a href="http://pandas.pydata.org/" target="_blank" rel="noopener">Pandas</a>, <a href="http://matplotlib.org/" target="_blank" rel="noopener">Matplotlib</a> 和 <a href="http://ipython.org/" target="_blank" rel="noopener">IPython</a> </p><p>《<a href="https://www.kevinsheppard.com/images/0/09/Python_introduction.pdf" target="_blank" rel="noopener">Introduction to Python for Econometrics, Statistics and Data Analysis</a>》</p><h4 id="2-从网站通过API挖掘和抓取数据"><a href="#2-从网站通过API挖掘和抓取数据" class="headerlink" title="2.从网站通过API挖掘和抓取数据"></a>2.从网站通过API挖掘和抓取数据</h4><ul><li><a href="https://geduldig.github.io/tutorials/twitter-couchdb/" target="_blank" rel="noopener">Mini-Tutorial: Saving Tweets to a Database with Python</a> （微型教程：使用Python保存推文到数据库）</li><li><a href="https://jessesw.com/Data-Science-Skills/" target="_blank" rel="noopener">Web Scraping Indeed for Key Data Science Job Skills</a> （网页抓取关键数据科学工作技巧）</li><li><a href="http://rodrigoaraujo.me/general/setup/demo/2015/07/02/Case-Study:-Sentiment-Analysis-on-Movie-Reviews.html" target="_blank" rel="noopener">Case Study: Sentiment Analysis On Movie Reviews</a> （案例学习：电影评论中的情感分析）</li><li><a href="https://first-web-scraper.readthedocs.org/en/latest/" target="_blank" rel="noopener">First Web Scraper</a> （第一网页抓取）</li><li><a href="https://indico.io/blog/email-sentiment/" target="_blank" rel="noopener">Sentiment Analysis of Emails</a> （邮件的情感分析）</li><li><a href="http://stevenloria.com/how-to-build-a-text-classification-system-with-python-and-textblob/?utm_content=buffera3613&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer" target="_blank" rel="noopener">Simple Text Classification</a> （简单文本分类）</li><li><a href="http://fjavieralba.com/basic-sentiment-analysis-with-python.html" target="_blank" rel="noopener">Basic Sentiment Analysis with Python</a> （Python基础情感分析）</li><li><a href="http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/" target="_blank" rel="noopener">Twitter sentiment analysis using Python and NLTK</a> （使用Python和NLTK 做Twitter情感分析）</li><li><a href="http://andybromberg.com/sentiment-analysis-python/" target="_blank" rel="noopener">Second Try: Sentiment Analysis in Python</a> （第二个尝试：Python情感分析）</li><li><a href="https://jessesw.com/NLP-Movie-Reviews/" target="_blank" rel="noopener">Natural Language Processing in a Kaggle Competition for Movie Reviews</a>  （电影评论相关Kaggle Competition中的NLP自然语言处理）</li></ul><h4 id="3-scikit-learn"><a href="#3-scikit-learn" class="headerlink" title="3. scikit-learn"></a>3. <a href="http://scikit-learn.org/stable/tutorial/index.html" target="_blank" rel="noopener">scikit-learn</a></h4><ul><li><a href="http://kukuruku.co/hub/python/introduction-to-machine-learning-with-python-andscikit-learn?utm_content=buffer70870&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer" target="_blank" rel="noopener">Introduction to Machine Learning with Python and Scikit-Learn</a> （机器学习中 Python 和 Scikit-Learn 的介绍）</li><li><a href="http://blog.yhathq.com/posts/data-science-in-python-tutorial.html" target="_blank" rel="noopener">Data Science in Python</a> （Python 中的数据科学）</li><li><a href="http://blog.yhathq.com/posts/machine-learning-for-predicting-bad-loans.html" target="_blank" rel="noopener">Machine Learning for Predicting Bad Loans</a> （用机器学习来预测坏账）</li><li><a href="http://rodrigoaraujo.me/general/setup/demo/2015/06/30/A-Generic-Architecture-for-Text-Classification.html" target="_blank" rel="noopener">A Generic Architecture for Text Classification with Machine Learning</a>     （通过机器学习来分类文本的通用架构）</li><li><a href="http://rodrigoaraujo.me/general/setup/demo/2015/03/31/using-python-and-AI-to-predict-types-of-wine.html" target="_blank" rel="noopener">Using Python and AI to predict types of wine</a>  （利用 Python 和 AI 人工智能来预测酒的品种）</li><li><a href="https://jmetzen.github.io/2015-01-29/ml_advice.html" target="_blank" rel="noopener">Advice for applying Machine Learning</a>  （应用机器学习的建议）</li><li><a href="http://blog.yhathq.com/posts/predicting-customer-churn-with-sklearn.html" target="_blank" rel="noopener">Predicting customer churn with scikit-learn</a>  （使用 scikit-learn 预测用户流失）</li><li><a href="http://www.christianpeccei.com/musicmap/" target="_blank" rel="noopener">Mapping Your Music Collection</a>  （映射你的音乐收藏）</li><li><a href="http://blog.yhathq.com/posts/data-science-in-python-tutorial.html" target="_blank" rel="noopener">Data Science in Python</a>  （Python 中的数据科学）</li><li><a href="http://rodrigoaraujo.me/general/setup/demo/2015/07/02/Case-Study:-Sentiment-Analysis-on-Movie-Reviews.html" target="_blank" rel="noopener">Case Study: Sentiment Analysis on Movie Reviews</a>  （案例学习：电影评论中的情感分析）</li><li><a href="http://nbviewer.ipython.org/github/brandomr/document_cluster/blob/master/cluster_analysis_web.ipynb" target="_blank" rel="noopener">Document Clustering with Python</a>  （Python中的文档聚类）</li><li><a href="http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/?utm_content=buffere3a18&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer" target="_blank" rel="noopener">Five most popular similarity measures implementation in python</a>  （5 个最流行的Python相似度测量的实现）</li><li><a href="http://rodrigoaraujo.me/general/setup/demo/2015/07/02/Case-Study:-Sentiment-Analysis-on-Movie-Reviews.html" target="_blank" rel="noopener">Case Study: Sentiment Analysis on Movie Reviews</a>  （案例学习：电影评论中的情感分析）</li><li><a href="http://slendermeans.org/pages/will-it-python.html" target="_blank" rel="noopener">Will it Python?</a>  （将会是 Python 么？）</li><li><a href="https://www.youtube.com/watch?v=Xv6exLVbGPk" target="_blank" rel="noopener">Text Processing in Machine Learning</a>  （机器学习中的文本处理）</li><li><a href="http://blog.francoismaillet.com/epic-celebration/" target="_blank" rel="noopener">Hacking an epic NHL goal celebration with a hue light show and real-time machine learning</a>   （使用色彩灯光秀和实时机器学习黑入史诗级 NHL（北美冰球联赛）进球庆祝）</li><li><a href="http://gabrielelanaro.github.io/blog/2015/04/19/room-prices-vancouver.html" target="_blank" rel="noopener">Vancouver Room Prices</a> （温哥华房间价格）</li><li><a href="https://jessesw.com/Faculty-Salaries/" target="_blank" rel="noopener">Exploring and Predicting University Faculty Salaries</a> （探索和预测大学教师工资）</li><li><a href="https://jessesw.com/Air-Delays/" target="_blank" rel="noopener">Predicting Airline Delays</a>  （预测航班延误）</li></ul><h4 id="4-理论"><a href="#4-理论" class="headerlink" title="4.理论"></a>4.理论</h4><p>周志华的西瓜书、李航的统计学习书</p><h4 id="5-关于机器学习的博客列表"><a href="#5-关于机器学习的博客列表" class="headerlink" title="5.关于机器学习的博客列表"></a>5.关于机器学习的博客列表</h4><p><a href="http://bigdata-madesimple.com/frequently-updated-machine-learning-blogs/?utm_source=email&amp;utm_medium=email&amp;utm_campaign=BDMS-newsletter-March27&amp;utm_content=BDMS-newsletter-March27+CID_904ac9851a52fa4c3720472ab62f69cd&amp;utm_source=Email&amp;utm_term=Read%20More" target="_blank" rel="noopener">List of frequently updated blogs</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;#机器学习列表&lt;/p&gt;
&lt;p&gt;参考链接&lt;a href=&quot;https://www.cnblogs.com/mcafee/p/6bbddd792d25d1e0e2a8859729b028e9.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;机器学习中的
      
    
    </summary>
    
      <category term="博客阅读" scheme="http://yoursite.com/categories/%E5%8D%9A%E5%AE%A2%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="学习列表" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0%E5%88%97%E8%A1%A8/"/>
    
  </entry>
  
  <entry>
    <title>SVM</title>
    <link href="http://yoursite.com/2018/04/27/SVM/"/>
    <id>http://yoursite.com/2018/04/27/SVM/</id>
    <published>2018-04-27T01:34:03.830Z</published>
    <updated>2018-04-27T01:36:15.476Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p>SVM在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。</p><p>支持向量机方法是建立在<code>统计学习理论的VC 维理论</code>和<code>结构风险最小原理</code>基础上的，根据有限的样本信息在模型的复杂性（即对特定训练样本的学习精度）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折衷，以期获得最好的推广能力[14]（或称泛化能力）。</p><p>所谓VC维是对函数类的一种度量，VC维越高，一个问题就越复杂。SVM解决问题的时候，和样本的维数是无关的（甚至样本是上万维的都可以，这使得SVM很适合用来解决文本分类的问题，当然，有这样的能力也因为引入了核函数）。</p><p>真实风险应该由<code>经验风险</code>和<code>置信风险</code>。置信风险与<code>样本数量</code>和<code>VC维</code></p><p>泛化误差界的公式为：</p><p>$R(w)\leq R_{emp}(w)+\phi(n/h)$    R(w)就是真实风险，Remp(w)就是经验风险，Ф(n/h)就是置信风险</p><p>统计学习的目标从经验风险最小化变为了寻求经验风险与置信风险的和最小，即结构风险最小。</p><hr><h4 id="线性函数"><a href="#线性函数" class="headerlink" title="线性函数"></a>线性函数</h4><p>有一线性函数$g(x)=wx+b$</p><ul><li>x样本的向量表示</li><li>这个形式并不局限于二维的情况，在n维空间中仍然可以使用这个表达式</li></ul><p>每一个样本由一个向量（就是那些文本特征所组成的向量）和一个标记（标示出这个样本属于哪个类别）组成。如下：</p><p>$D_i=(x_i,y_i)$    xi就是文本向量（维数很高），yi就是分类标记。</p><p>在二元的线性分类中，这个表示分类的标记只有两个值，1和-1。我们就可以定义一个样本点到某个超平面的<code>间隔</code>：</p><p>$\delta_i=y_i(wx_i+b)$</p><p>归一化，<code>几何间隔</code>：</p><p>$\delta_i=\frac{1}{||w||}|g(x_i)|$    <code>没错，这不就是解析几何中点xi到直线g(x)=0的距离公式嘛！</code></p><p>当用归一化的w和b代替原值之后的间隔有一个专门的名称，叫做<code>几何间隔</code>-所表示的正是点到超平面的欧氏距离。</p><p><img src="http://p7tk13vbh.bkt.clouddn.com/3.png" alt="3"></p><p>几何间隔越大的解，它的误差上界越小。<code>最大化几何间隔与最小化||w||完全是一回事。</code></p><p>我们常用的方法并不是固定||w||的大小而寻求最大几何间隔，而是<code>固定间隔（例如固定为1），寻找最小的||w||</code>。</p><hr><h4 id="规划"><a href="#规划" class="headerlink" title="规划"></a>规划</h4><p>$ min f(x)$     </p><p>$subject to ,C_i(x)\leq0,i=1,2,…,p$    </p><p>​        $C_j(x)=0,j=p+1,p+2,…,p+q$</p><p>要求f(x)在哪一点上取得最小值，但不是在整个空间里找，而是在约束条件所划定的一个有限的空间里找，这个有限的空间就是优化理论里所说的<code>可行域</code>。</p><p>$min\frac{1}{2}||w||^2 ,subject  to$   $y_i[(wx_i)+b]\geq1  (i=1,2,…,l)(l是样本数)$</p><p>这种规划问题称为<code>二次规划</code>,由于它的可行域是一个凸集，因此它是一个<code>凸二次规划</code>。</p><p>凸二次规划让人喜欢的地方就在于，它有解，且常常加限定成分，说它有<code>全局最优解</code></p><hr><h4 id="拉格朗日乘子"><a href="#拉格朗日乘子" class="headerlink" title="拉格朗日乘子"></a>拉格朗日乘子</h4><p>样本确定了w，用数学的语言描述，就是w可以表示为样本的某种组合：</p><p>$w=a_1x_1+a_2x_2+…+a_nx_n$</p><p>式子中的αi是一个一个的数（在严格的证明过程中，这些α被称为<code>拉格朗日乘子</code>），而xi是样本点，因而是向量，n就是总样本点的个数。</p><p>因此g(x)的表达式严格的形式应该是：</p><p>$g(x)=&lt;w,x&gt;+b$</p><p>$w=a_1y_1x_1+a_2y_2x_2+…+a_ny_nx_n$    简化：$w=\sum_{i=1}^{n}(a_iy_ix_i)$</p><p>原式为$g(x)=\sum_{i=1}^{n}a_iy_i&lt;x_i,x&gt;+b$</p><p>以这样的形式描述问题以后，我们的优化问题少了很大一部分不等式约束</p><hr><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>参考链接<a href="http://www.blogjava.net/zhenandaci/" target="_blank" rel="noopener">Jasper’s Java Jacal</a></p><p>提供的样本线性不可分，线性分类器求解程序会无限循环，如何将不可分数据变得线性可分？</p><p>二维空间的线性函数（直线）不能将两类正确分开。但曲线可以，问题是它不是一个线性函数。</p><p>$y=[ \begin{matrix}  y_1 \  y_2\  y_3\end{matrix}]=[ \begin{matrix}  1 \ x\ x^2\end{matrix}]$,$a=[ \begin{matrix}  a_1 \  a_2\  a_3\end{matrix}]=[ \begin{matrix}  c_0 \  c_1\  c_2\end{matrix}]$   </p><p>对于$g(x)=c_0+c_1x+c_2x^2$</p><p>这样g(x)可以转化为$f(y)=&lt;a,y&gt;$ <code>在任意维度空间，这种形式的函数都是一个线性函数</code></p><p><code>原来在二维空间中一个线性不可分的问题，映射到四维空间后，变成了线性可分的！</code></p><p>例如：我们文本问题的原始空间是1000维（即每个被分类的文档被表示为一个1000维的向量），现在我们有一个2000维空间里的线性函数    $f(x’)=<w',x'>+b$ ,其中w’和x’都为2000维向量</w',x'></p><p><code>分类的过程是先把x变换为2000维的向量x’，然后求这个变换后的向量x’与向量w’的内积，再把这个内积的值和b相加，就得到了结果，看结果大于阈值还是小于阈值就得到了分类结果。</code></p><p>线性分类器：$f(x’)=\sum_{i=1}^{n} a_iy_i&lt;x_i’,x’&gt;+b​$</p><p>用低维空间里的函数代替：$g(x)=\sum_{i=1}^{n}a_iy_iK(x_i,x)+b$</p><p>直接拿低维的输入向g(x)代入，<code>核函数K()的作用是接受两个低维空间里的向量，能够计算出经过某个变换后在高维空间里的向量内积值。</code></p><hr><h2 id="核函数的选择-松弛变量"><a href="#核函数的选择-松弛变量" class="headerlink" title="核函数的选择-松弛变量"></a>核函数的选择-松弛变量</h2><p><code>“硬间隔”分类法</code>：有噪声的情况会使得整个问题无解</p><p>$y_i[(wx_i)+b]\geq1$引入容错性，给1这个硬性的阈值加一个松弛变量$\xi_i\geq0$,$y_i[(wx_i)+b]\geq1-\xi_i$</p><p>原始的硬间隔分类对应的优化问题：</p><p>$min\frac{1}{2}||w||^2 ,subject  to$   $y_i[(wx_i)+b]\geq1  (i=1,2,…,l)(l是样本数)$</p><p>把损失加入到目标函数中，就需要一个惩罚因子C，优化问题变为：</p><p>$min\frac{1}{2}||w||^2+C\sum_{i=1}^{l}\xi_i ,subject  to$   $y_i[(wx_i)+b]\geq1-\xi_i  (i=1,2,…,l)(l是样本数),\xi_i\geq0$</p><p>但需要<code>注意</code>:</p><ul><li>并非所有的样本点都有一个松弛变量与其对应，实际上只有“离群点”才有。</li><li>松弛变量的值实际上标示出了对应的点到底离群有多远，值越大，点就越远。</li><li>惩罚因子C决定了你有多重视离群点带来的损失，当所有离群点的松弛变量的和一定时,你定的C越大，对目标函数的损失也越大，此时就暗示着你非常不愿意放弃这些离群点，最极端的情况是你把C定为无限大，这样只要稍有一个点离群，目标函数的值马上变成无限大，马上让问题变成无解，这就退化成了硬间隔问题。</li><li>C是一个你必须事先指定的值，指定这个值以后，解一下，得到一个分类器，然后用测试数据看看结果怎么样，如果不够好，换一个C的值。<code>优化问题在解的过程中，C一直是定值</code></li></ul><p>在原始的低维空间中，样本相当的不可分，无论你怎么找分类平面，总会有大量的离群点，此时用<code>核函数</code>向高维空间映射一下，虽然结果仍然是不可分的，但比原始空间里的要更加接近线性可分的状态（就是达到了近似线性可分的状态），此时再用<code>松弛变量</code>处理那些少数“冥顽不化”的离群点，就简单有效得多啦。</p><hr><h4 id="松弛变量-数据集偏斜"><a href="#松弛变量-数据集偏斜" class="headerlink" title="松弛变量-数据集偏斜"></a>松弛变量-数据集偏斜</h4><p>对付数据集偏斜问题的方法之一就是在<code>惩罚因子</code>上作文章，那就是给样本数量少的负类更大的惩罚因子，表示我们重视这部分样本</p><p>$C_+\sum_{i=1}^{p}\xi_i+C_-\sum_{j=p+1}^{p+q}\xi_j$$,\xi_i\geq0$</p><p>其中i=1…p都是正样本，j=p+1…p+q都是负样本</p><hr><h4 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h4><p>SVM是一种典型的两类分类器。</p><p>所谓<code>“一类对其余”</code>的方法，就是每次仍然解一个两类分类的问题，但会出现<code>分类重叠现象</code>和<code>不可分类现象</code></p><ul><li>把类别1的样本定为正样本，其余2，3，4，5的样本合起来定为负样本，这样得到一个两类分类器，它能够指出一篇文章是还是不是第1类的</li></ul><p><code>一对一</code>的方法，就是每次选一个类的样本作正类样本，而负类样本则变成只选一个类</p><ul><li>第一个只回答“是第1类还是第2类”</li></ul><p><code>DAG SVM</code>的方法是按下图组织分类器，但会存在错误向下累积的现象</p><p><img src="C:\Users\xiaoJane\blog\public\images\DAG_SVM.png" alt="DAG_SVM"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;SVM&quot;&gt;&lt;a href=&quot;#SVM&quot; class=&quot;headerlink&quot; title=&quot;SVM&quot;&gt;&lt;/a&gt;SVM&lt;/h2&gt;&lt;p&gt;SVM在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。&lt;/p&gt;
&lt;p&gt;支持向
      
    
    </summary>
    
      <category term="博客阅读" scheme="http://yoursite.com/categories/%E5%8D%9A%E5%AE%A2%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>搭建Blog</title>
    <link href="http://yoursite.com/2018/04/20/%E6%90%AD%E5%BB%BABlog/"/>
    <id>http://yoursite.com/2018/04/20/搭建Blog/</id>
    <published>2018-04-20T11:26:20.861Z</published>
    <updated>2018-04-27T01:30:56.737Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Markdown"><a href="#Markdown" class="headerlink" title="Markdown"></a>Markdown</h2><p>可参考线上编辑器<a href="https://www.zybuluo.com/mdeditor?url=https%3A%2F%2Fwww.zybuluo.com%2Fstatic%2Feditor%2Fmd-help.markdown" target="_blank" rel="noopener">Cmd Markdown</a>和<a href="http://mahua.jser.me/" target="_blank" rel="noopener">MaHua</a></p><h2 id="Hexo-github-Theme搭建Blog"><a href="#Hexo-github-Theme搭建Blog" class="headerlink" title="Hexo+github+Theme搭建Blog"></a>Hexo+github+Theme搭建Blog</h2><hr><p>参考链接<a href="http://yangbingdong.com/2017/build-blog-hexo-base/" target="_blank" rel="noopener">基于Hexo+Github+Coding搭建个人博客</a>和<a href="https://www.cnblogs.com/fengxiongZz/p/7707219.html" target="_blank" rel="noopener">Hexo+Github一步步搭建博客</a></p><h3 id="1-配置Node-js和git"><a href="#1-配置Node-js和git" class="headerlink" title="1.配置Node.js和git"></a>1.配置<a href="https://nodejs.org/zh-cn/" target="_blank" rel="noopener">Node.js</a>和git</h3><h3 id="2-Github新建项目-账户名-github-io"><a href="#2-Github新建项目-账户名-github-io" class="headerlink" title="2.Github新建项目:账户名.github.io"></a>2.Github新建项目:账户名.github.io</h3><h3 id="3-Hexo与Github-page联系起来"><a href="#3-Hexo与Github-page联系起来" class="headerlink" title="3.Hexo与Github page联系起来"></a>3.Hexo与Github page联系起来</h3><p><strong>生成SSH Key</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xxxxxxx\blog&gt;</span><br><span class="line">$ git config --global user.name “github的账户名”</span><br><span class="line">$ git config --global user.email “github的邮箱名”</span><br></pre></td></tr></table></figure><p>执行后在blog文件夹会新建一个文件夹/.ssh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C “github的邮箱名”</span><br></pre></td></tr></table></figure><p>连续三个回车，生成密钥，最后在/.ssh下生成了两个文件：id_rsa和id_rsa.pub</p><p><strong>添加SSH Key</strong></p><p>登录github，点击头像下的setting，添加ssh</p><p>新建一个new ssh key，将id_rsa.pub文件里的内容复制上去，其中windows默认读取私钥位置<code>/.ssh/id_rsa</code></p><p><strong>验证</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh -T git@github.com</span><br></pre></td></tr></table></figure><h3 id="4-安装Hexo"><a href="#4-安装Hexo" class="headerlink" title="4.安装Hexo"></a>4.安装Hexo</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo -g#下载Hexo</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init#初始化文件夹</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ npm install#安装所需组件</span><br><span class="line">$ npm install hexo-server --save</span><br><span class="line">$ npm install hexo-admin --save</span><br><span class="line">$ npm install hexo-generator-archive --save</span><br><span class="line">$ npm install hexo-generator-feed --save</span><br><span class="line">$ npm install hexo-generator-search --save</span><br><span class="line">$ npm install hexo-generator-tag --save</span><br><span class="line">$ npm install hexo-deployer-git --save</span><br><span class="line">$ npm install hexo-generator-sitemap --save</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g#hexo generate，生成静态文件</span><br><span class="line">$ hexo s#hexo server,在本地服务器运行</span><br></pre></td></tr></table></figure><p>##5.部署到Github</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 清除、生成、启动</span><br><span class="line">$ hexo clean &amp;&amp; hexo g -s</span><br><span class="line"># 清除、生成、部署</span><br><span class="line">$ hexo clean &amp;&amp; hexo g -d</span><br></pre></td></tr></table></figure><h2 id="6-更换主题"><a href="#6-更换主题" class="headerlink" title="6.更换主题"></a>6.更换主题</h2><p><a href="https://www.zhihu.com/question/24422335" target="_blank" rel="noopener">主题</a></p><p><strong>复制主题</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd your-hexo-site</span><br><span class="line">git clone https://github.com/iissnan/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure><p><strong>启用主题</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: next#_config.yml</span><br></pre></td></tr></table></figure><h2 id="7-插入公式"><a href="#7-插入公式" class="headerlink" title="7.插入公式"></a>7.插入公式</h2><p><a href="https://en.wikibooks.org/wiki/LaTeX/Mathematics" target="_blank" rel="noopener">Latex</a></p><h2 id="8-添加分类及标签"><a href="#8-添加分类及标签" class="headerlink" title="8.添加分类及标签"></a>8.添加分类及标签</h2><p><a href="https://linlif.github.io/2017/05/27/Hexo%E4%BD%BF%E7%94%A8%E6%94%BB%E7%95%A5-%E6%B7%BB%E5%8A%A0%E5%88%86%E7%B1%BB%E5%8F%8A%E6%A0%87%E7%AD%BE/" target="_blank" rel="noopener">Hexo使用攻略-添加分类及标签</a></p><p>##9.LaTex公式/编号/对齐</p><p><a href="https://www.zybuluo.com/fyywy520/note/82980#%E5%9D%97%E7%BA%A7%E5%85%AC%E5%BC%8F%E7%9A%84%E7%BC%96%E5%8F%B7" target="_blank" rel="noopener">Markdown下LaTex公式编号对齐</a></p><h2 id="10-图片存储"><a href="#10-图片存储" class="headerlink" title="10.图片存储"></a>10.图片存储</h2><p><a href="https://developer.qiniu.com/kodo/sdk/1240/objc" target="_blank" rel="noopener">七牛云</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Markdown&quot;&gt;&lt;a href=&quot;#Markdown&quot; class=&quot;headerlink&quot; title=&quot;Markdown&quot;&gt;&lt;/a&gt;Markdown&lt;/h2&gt;&lt;p&gt;可参考线上编辑器&lt;a href=&quot;https://www.zybuluo.com/mdedi
      
    
    </summary>
    
    
  </entry>
  
</feed>
